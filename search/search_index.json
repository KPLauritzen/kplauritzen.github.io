{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What I've been up to","text":""},{"location":"about/","title":"About me","text":"<p>Kasper Primdal Lauritzen Machine Learning Engineer at Vestas</p>"},{"location":"about/#funsilly-projects","title":"Fun/silly projects","text":"<ul> <li>World Cup 2018 predictions</li> <li>AirBnb but for bathrooms - POOPR</li> <li>Foosball tracking for financial institutions - Moneyball</li> </ul>"},{"location":"about/#experience","title":"Experience","text":"<ul> <li> <p>Machine Learning Engineer, 2024-present,</p> <p>AI Center of Enablement at Vestas.</p> </li> <li> <p>Data Scientist, 2019-2024,</p> <p>Data and Analytics at DFDS. Deploying, monitoring, packaging, and testing machine learning models.</p> </li> <li> <p>Data Scientist, 2017-2019,</p> <p>Pricing Excellence at Danske Bank. Predicting customer behaviour and optimizing interest rates for bank loans in several Nordic markets.</p> </li> <li> <p>PhD in Computational Chemistry, 2014-2017,</p> <p>Group of Gemma Solomon, University of Copenhagen. Predicting structual properties of nano-scale systems.</p> <p>Publications</p> </li> </ul>"},{"location":"about/#contact","title":"Contact","text":"<ul> <li>GitHub: KPLauritzen</li> <li>LinkedIn: KPLauritzen</li> <li>Mastodon: @KPLauritzen</li> </ul>"},{"location":"feeds/","title":"RSS Feeds","text":"<p>I have added RSS feeds to the blog. You can subscribe to the feed using your favorite RSS reader.</p> <ul> <li>RSS Feed</li> <li>JSON Feed</li> </ul>"},{"location":"recommendations/","title":"Blogroll","text":"<p>Here are some blogs I like.</p>"},{"location":"recommendations/#my-friends","title":"My Friends","text":"<ul> <li>Jimmy Kromann + Peter Ericson</li> <li>Morten Hels</li> <li>Markus W\u00fcstenberg</li> <li>Zeth Odderskov</li> <li>Willi Carlsen</li> <li>Fred</li> </ul>"},{"location":"recommendations/#internet-people","title":"Internet people","text":"<ul> <li>Simon Willison</li> <li>Scott Alexander at Astral Codex Ten</li> <li>Julia Evans</li> <li>Gwern Branwen</li> <li>Zvi Mowshowitz</li> </ul>"},{"location":"2018-06-13-worldcup/","title":"Predicting the 2018 FIFA World Cup","text":"<p><code>TL;DR</code>: Check the predictions at the bottom of the page, and see the code on GitHub.</p> <p>The FIFA World Cup 2018 is starting tomorrow in Russia. I joined a competition at work to predict the final score of all the games of the tournament, but since I don't know anything about football (and do know something about machine learning), I thought it would be fun to build a simple model to do the predictions.</p>"},{"location":"2018-06-13-worldcup/#the-challenge","title":"The challenge","text":"<p>I want to guess the final score of all matches. There is partial credit for just guessing the match winner, so if it is not possible to guess the final score I want to err towards guessing the winner.</p> <p>I see two main challenges:</p> <ol> <li> <p>I have to predict how the national teams will perform. Each team usually plays less than 10 games a year, so there is not a lot of data to go on.</p> </li> <li> <p>I have to predict scores instead of just outcomes. So this is not a classification problem, but instead a regression problem. I'm not so confident in that class of machine learning problems, so this will be a fun exercise.</p> </li> </ol>"},{"location":"2018-06-13-worldcup/#initial-thoughts","title":"Initial thoughts","text":"<p>I will be using the team ratings from the game FIFA 18 as my training features. They are available for both international and club games and they are updated every week. For each team there are ratings for attacking ('ATT'), midfield ('MID'), defending ('DEF') and overall ('OVR'). MY hypothesis is that if I find a pattern in how many goals a 85 ATT teams scores against a 74 DEF team, this will hold for both clubs and international teams. In this way I will be able to train on many games from for example premier league.</p> <p>Additionally, I will include the implied odds from bookmaker odds for each match outcome. I hope this will make it more likely to predict the correct winner as this also gives some points in my office tournament.</p>"},{"location":"2018-06-13-worldcup/#data-sources","title":"Data Sources","text":"<p>I found FIFA team ratings at FIFA index. This was not available as a download, so I had to scrape the ratings.</p> <p>The final scores of matches for several European leagues are available at Football-data.co.uk. Here the data is available as a <code>.csv</code> file, so this couldn't be easier. It even includes bookmaker odds!</p> <p>The odds from the world cup games was downloaded and scraped from Odds Checker.</p> <p>The fixtures for the world cup was downloaded from Fixture Download.</p>"},{"location":"2018-06-13-worldcup/#my-approach","title":"My approach","text":"<p>I wanted to use this as an opportunity to use luigi for handling the data dependencies. This is definitely overkill for a simple project like this, but it was still a good testing grounds. To illustrate, here is a portion of the final dependency graph for creating the training dataset.</p> <p></p> <p>You can check out the code at my GitHub.</p> <p>To summarize the training data creation:</p> <ol> <li> <p>For a certain league (e.g. Premier League) I download the weekly team ratings pages from FIFA Index, using <code>urllib</code>.</p> </li> <li> <p>I extract the ratings for each team with <code>beautifulsoup4</code>.</p> </li> <li> <p>I download the results of all games in the league from football-data. This dataset includes bookmaker odds. I use the Bet365 odds.</p> </li> <li> <p>I merge the ratings for home and away teams onto the results, using <code>pandas</code>. I merge on team name and date.</p> <ul> <li>NB: The team names are not consistent between the two data sources, so I had to normalize the names to be able to join on them.</li> </ul> </li> <li> <p>To avoid having to account for home field advantage (which won't be a thing at the world cup - except for Russia), I add all matches AGAIN with home and away team reversed. This should make home and away teams symmetric.</p> </li> <li> <p>Repeat steps 1-5 for other leagues if desired. It could be beneficial to include lower tier leagues to have examples of ratings like the lower tier national teams.</p> </li> </ol>"},{"location":"2018-06-13-worldcup/#training","title":"Training","text":"<p>I've not really explored the possibilities very carefully here. I've trained a Ridge regression in <code>scikit-learn</code> with home AND away goals as the targets. It works the best of all the methods I tried with a mean absolute error of about 0.9. So on average my predictions are about 1 goal off the correct answer.</p>"},{"location":"2018-06-13-worldcup/#prediction-data","title":"Prediction data","text":"<p>The data gathering setup is very similar to the process described above for club games.</p> <p>One issue is that the predictions are not integers but floats. How do you interpret a prediction of 1.43 against 1.18? Should that be rounded to 1-1? How about 0.6 against 1.4? Is it still a 1-1 if I predict one team to have 0.8 goals advantage?</p> <p>I think there is a smart answer to this, maybe using the bookmaker odds to decide if this should be a draw or if one team winning is more likely.</p> <p>I haven't figured this out, and I would love some more input. Please reach out if you have an idea.</p>"},{"location":"2018-06-13-worldcup/#actual-predictions","title":"Actual predictions","text":"<p>No more beating around the bush. Here are my final predictions:</p> <pre><code>| team_home    | goals_home | goals_away | team_away    |\n|--------------|------------|------------|--------------|\n| Russia       |          2 |          1 | Saudi Arabia |\n| Egypt        |          0 |          2 | Uruguay      |\n| Morocco      |          1 |          1 | Iran         |\n| Portugal     |          1 |          2 | Spain        |\n| France       |          2 |          1 | Australia    |\n| Argentina    |          2 |          1 | Iceland      |\n| Peru         |          1 |          1 | Denmark      |\n| Croatia      |          2 |          1 | Nigeria      |\n| Costa Rica   |          1 |          1 | Serbia       |\n| Germany      |          2 |          1 | Mexico       |\n| Brazil       |          2 |          1 | Switzerland  |\n| Sweden       |          1 |          1 | South Korea  |\n| Belgium      |          2 |          1 | Panama       |\n| Tunisia      |          1 |          2 | England      |\n| Colombia     |          2 |          1 | Japan        |\n| Poland       |          1 |          1 | Senegal      |\n| Russia       |          1 |          1 | Egypt        |\n| Portugal     |          2 |          1 | Morocco      |\n| Uruguay      |          2 |          0 | Saudi Arabia |\n| Iran         |          1 |          2 | Spain        |\n| Denmark      |          2 |          1 | Australia    |\n| France       |          2 |          1 | Peru         |\n| Argentina    |          2 |          1 | Croatia      |\n| Brazil       |          2 |          0 | Costa Rica   |\n| Nigeria      |          1 |          1 | Iceland      |\n| Serbia       |          1 |          1 | Switzerland  |\n| Belgium      |          2 |          1 | Tunisia      |\n| South Korea  |          1 |          2 | Mexico       |\n| Germany      |          2 |          1 | Sweden       |\n| England      |          2 |          1 | Panama       |\n| Japan        |          1 |          1 | Senegal      |\n| Poland       |          1 |          1 | Colombia     |\n| Uruguay      |          1 |          1 | Russia       |\n| Saudi Arabia |          1 |          2 | Egypt        |\n| Iran         |          1 |          2 | Portugal     |\n| Spain        |          2 |          1 | Morocco      |\n| Denmark      |          1 |          2 | France       |\n| Australia    |          1 |          1 | Peru         |\n| Nigeria      |          1 |          2 | Argentina    |\n| Iceland      |          1 |          2 | Croatia      |\n| Mexico       |          1 |          1 | Sweden       |\n| South Korea  |          1 |          2 | Germany      |\n| Serbia       |          1 |          2 | Brazil       |\n| Switzerland  |          1 |          1 | Costa Rica   |\n| Japan        |          1 |          1 | Poland       |\n| Senegal      |          1 |          1 | Colombia     |\n| Panama       |          1 |          1 | Tunisia      |\n| England      |          1 |          1 | Belgium      |\n</code></pre>"},{"location":"2018-06-13-worldcup/#related-work","title":"Related work","text":"<p>There is a very nice paper by Andreas Groll et al., doing the same thing but better and with much more rigor.</p> <p>And this great tweet in response to the paper:</p> <p>Your first mistake was using fifa rankings...they are an absolute horrible indicator of a countrys actual ability</p> <p>Marko Bilal (@markobilal) on Twitter</p> <p>Of course, the ever interesting FiveThirtyEight have also tried their hand at predicting the world cup. They also publish their methodology and their data. How can you not love them?</p>"},{"location":"2019-02-27-thistextdoesnotexist/","title":"This text does not exist","text":"<p><code>TL;DR</code>: I stole a thing.</p>"},{"location":"2019-02-27-thistextdoesnotexist/#generating-text-for-fun-and-profit","title":"Generating text for fun and profit","text":"<p>I stole samples from the recent OpenAI language model, generating text without human input. This continues the excellent work of many others.</p> <p>Many thanks to Peter Ericson for helping me bodge some javascript together for this project. I have absolutely no idea what I'm doing, so anything that remotely works is entirely Peters fault.</p> <p>See the thing here and we also bought thistextdoesnotexist.com because that seemed reasonable at the time.</p>"},{"location":"2021-08-11-convert-dataclasss-np-array/","title":"Converting between custom dataclasses and numpy arrays","text":"<p><code>TL;DR</code>: Implement <code>__array__()</code>, <code>__len__()</code> and <code>__getitem__()</code> methods on your <code>dataclass</code>. See the final section for a working example.</p> <p>I have gotten increasingly interested in python typehints, and in a recent project I'm creating a lot of custom types to create interfaces for different modules in my application. I usually try to keep the types as standardlib python types, but the <code>dataclass</code> can be pretty neat.</p> <p>Here is an example of a simple custom dataclass</p> <pre><code>from dataclasses import dataclass\n@dataclass\nclass Point2D:\n    x: float\n    y: float\n</code></pre> <p>If I want a simple way to convert this to a <code>numpy</code> array, I run into a few stumbling blocks:</p>"},{"location":"2021-08-11-convert-dataclasss-np-array/#converting-one-instance-to-a-nparray-the-naive-way","title":"Converting one instance to a np.array (the naive way)","text":"<pre><code>import numpy as np\n\np = Point2D(x=0.2, y=3.0)\narr = np.array(p)\nprint(arr, type(arr), arr.dtype)\n# Point2D(x=0.2, y=3.0) &lt;class 'numpy.ndarray'&gt; object\n</code></pre> <p>I don't get the values from <code>Point2D</code>, I just get an array with the object inside. However, we can implement an <code>__array__</code> method on <code>Point2D</code> that will allow numpy to produce an array with the correct dtype.</p> <pre><code>@dataclass\nclass Point2D:\n    ...\n    def __array__(self):\n        return np.array([self.x, self.y])\n</code></pre> <p>Now we get a much more sensible result when converting</p> <pre><code>p = Point2D(x=0.2, y=3.0)\narr = np.array(p)\nprint(arr, type(arr), arr.dtype)\n# [0.2 3. ] &lt;class 'numpy.ndarray'&gt; float64\n</code></pre> <p>The trouble comes when we want to make a new custom type that inherits from <code>Point2D</code>.</p>"},{"location":"2021-08-11-convert-dataclasss-np-array/#inheriting-the-__array__-method","title":"Inheriting the <code>__array__</code> method","text":"<p>Let's make a simple extension of <code>Point2D</code> to 3 dimensions</p> <pre><code>@dataclass\nclass Point3D(Point2D):\n    z: float\n</code></pre> <p>If we try to convert this into a numpy array, we run into trouble</p> <pre><code>p = Point3D(x=0.2, y=3.0, z=-1.0)\narr = np.array(p)\nprint(arr, type(arr), arr.dtype)\n# [0.2 3. ] &lt;class 'numpy.ndarray'&gt; float64\n</code></pre> <p>We are missing the new z dimension!</p> <p>One fix is to make a new <code>__array__</code> method.</p> <pre><code>@dataclass\nclass Point3D(Point2D):\n    ...\n    def __array__(self):\n        return np.array([self.x, self.y, self.z])\n</code></pre> <p>That will definitely work, but it breaks the DRY principle. Instead, we can make use of <code>dataclasses.astuple</code></p> <pre><code>from dataclasses import astuple\n\n@dataclass\nclass Point2D:\n    x: float\n    y: float\n    def __array__(self):\n        return np.array(astuple(self))\n\n@dataclass\nclass Point3D(Point2D):\n    z: float\n\np = Point3D(x=0.2, y=3.0, z=-1.0)\narr = np.array(p)\nprint(arr, type(arr), arr.dtype)\n# [ 0.2  3.  -1. ] &lt;class 'numpy.ndarray'&gt; float64\n</code></pre> <p>Less repetition and less chance of mistakes. Nice.</p> <p>Our next issue is when dealing with more than one instance of these custom classes at a time.</p>"},{"location":"2021-08-11-convert-dataclasss-np-array/#converting-lists-of-custom-dataclasses-with-nested-conversion","title":"Converting lists of custom dataclasses with nested conversion","text":"<p>If I have a few <code>Point</code>s, I might want a 2D np.array with all the values. The naive approach would be to do</p> <pre><code>p1 = Point3D(1, 2, 3)\np2 = Point3D(4, 5, 6)\nlist_of_points = [p1, p2] \narr = np.array(list_of_points)\nprint(arr, type(arr), arr.dtype, arr.shape)\n# [Point3D(x=1, y=2, z=3) Point3D(x=4, y=5, z=6)] &lt;class 'numpy.ndarray'&gt; object (2,)\n</code></pre> <p>Not only do I not get what I expected, I even get a bunch of warnings from numpy that this is a no-go</p> <pre><code>&lt;input&gt;:3: FutureWarning: The input object of type 'Point3D' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Point3D', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n&lt;input&gt;:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n</code></pre> <p>We already know we can get a numpy array from a single instance, so we can get around this hurdle with a simple list comprehension</p> <pre><code>arr = np.array([np.array(p) for p in list_of_points])\nprint(arr, type(arr), arr.dtype, arr.shape)\n# [[1 2 3]\n# [4 5 6]] &lt;class 'numpy.ndarray'&gt; int32 (2, 3)\n</code></pre> <p>That works, but it feels more like a workaround than a real solution. Should I really have to remember to do this nested conversion every time I want to get my data in a 2D matrix?</p> <p>No, if I just implement two additional methods on the base class, I don't have to think about this any more.</p>"},{"location":"2021-08-11-convert-dataclasss-np-array/#converting-lists-of-custom-dataclasses-with-__len__-and-__getitem__","title":"Converting lists of custom dataclasses with <code>__len__</code> and <code>__getitem__</code>","text":"<pre><code>from dataclasses import dataclass, astuple\nimport numpy as np\n\n@dataclass\nclass Point2D:\n    x: float\n    y: float\n\n    def __array__(self):\n        return np.array(astuple(self))\n\n    def __len__(self):\n        return astuple(self).__len__()\n\n    def __getitem__(self, item):\n        return astuple(self).__getitem__(item)\n\n@dataclass\nclass Point3D(Point2D):\n    z: float\n\np1 = Point3D(1, 2, 3)\np2 = Point3D(4, 5, 6)\nlist_of_points = [p1, p2] \narr = np.array(list_of_points)\nprint(arr, type(arr), arr.dtype, arr.shape)\n# [[1 2 3]\n# [4 5 6]] &lt;class 'numpy.ndarray'&gt; int32 (2, 3)\n</code></pre> <p>We are again abusing <code>dataclass.astuple</code> to let us access each class variable programatically, in order.</p> <p>To be honest, I don't really understand why <code>__array__</code> does not work for lists of custom dataclasses, but <code>__len__</code> and <code>__getitem__</code> does. If numpy is looping through each element one at a time to add it to an array, we might run into some performance issues at some point.</p> <p>But, for now, this looks fairly clean for my taste and it is very practical.</p>"},{"location":"2021-08-26-precommit/","title":"Use pre-commit to save time and avoid mistakes","text":"<p>I'm working in a team of data scientists, and most of us don't have a \"proper\" software background. Most here have some sort of natural sciences education and have picked up machine learning and software development along the way. This means that we don't have the same software craftmanship foundation to build from when our ML models need to grow, scale, and change.</p> <p>There is a lot of ways to improve in this area, but a simple one to implement for a whole team in one go is to require <code>pre-commit</code> installed in all projects. This is a tool that lets you define a set of checks that are performed on your code every time you make a commit in git (you are using git, right?).</p>"},{"location":"2021-08-26-precommit/#installation","title":"Installation","text":"<p>Make (or copy from below) a file called <code>.pre-commit-config.yaml</code> and place it in the root of your repository. Then</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"2021-08-26-precommit/#run","title":"Run","text":"<p>Every time you <code>git commit</code> the hooks you have defined in <code>.pre-commit-config.yaml</code> will be run on the changed files.</p> <p>If for some reason you want to run the hooks on all files (for instance in your CI/CD) pipeline, you can do</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"2021-08-26-precommit/#individual-checks","title":"Individual checks","text":""},{"location":"2021-08-26-precommit/#stop-dealing-with-whitespace-diffs-in-your-prs","title":"Stop dealing with whitespace diffs in your PRs","text":"<pre><code>-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: end-of-file-fixer\n    -   id: trailing-whitespace\n-   repo: https://github.com/pycqa/isort\n    rev: 5.8.0\n    hooks:\n    - id: isort\n      name: isort\n</code></pre> <p>The two first hooks fixes small whitespace mistakes. Each file should end with just a newline, and there should be no whitespace at the end of a line.</p> <p><code>isort</code> sorts your import statements. It is a minor thing, but it will group imports into 3 groups:</p> <ol> <li>Included in Python stdlib.</li> <li>Third party library.</li> <li>Local code.</li> </ol> <p>There is some setup needed to make it compatible with <code>black</code>. See Full setup for details.</p>"},{"location":"2021-08-26-precommit/#you-probably-committed-this-by-mistake","title":"You probably committed this by mistake","text":"<pre><code>-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: check-ast\n    -   id: check-json\n    -   id: check-yaml\n    -   id: debug-statements\n    -   id: detect-aws-credentials\n        args: [--allow-missing-credentials]\n    -   id: detect-private-key\n    -   id: check-merge-conflict\n    -   id: check-added-large-files\n        args: ['--maxkb=3000']\n</code></pre> <p>Here is a bunch of hooks that will</p> <ul> <li> <p>Check if your Python code is valid (avoiding those <code>SyntaxError</code>s that sometimes crop up)</p> </li> <li> <p>Check that json and yaml files can be parsed</p> </li> <li> <p>Check that you don't have any leftover <code>breakpoint()</code> statements from a debugging session.</p> </li> <li> <p>Check that you haven't accidentally committed secrets.</p> </li> <li> <p>Check that you haven't committed an unresolved merge conflict, like leaving</p> <pre><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; HEAD\n</code></pre> <p>in the file.</p> </li> <li> <p>Check that you haven't committed an unusally large file. If you actually need large files inside your repo, use git-lfs.</p> </li> </ul>"},{"location":"2021-08-26-precommit/#make-jupyter-notebook-diffs-easier-to-deal-with","title":"Make Jupyter Notebook diffs easier to deal with","text":"<pre><code>-   repo: https://github.com/kynan/nbstripout\n    rev: 0.5.0\n    hooks:\n    - id: nbstripout\n</code></pre> <p><code>nbstripout</code> is very useful if you commit a lot of Jupyter Notebooks to your repo. The output cells are saved in the file, so if you are outputting some large plots, each notebook can become quite big. If your notebooks are not just one-off explorations, but you come back to them more than once, this will make the PR diffs much easier to read.</p> <p>If that is NOT the case, maybe you don't want or need this one.</p>"},{"location":"2021-08-26-precommit/#stop-arguing-over-code-style","title":"Stop arguing over code style","text":"<pre><code>-   repo: https://github.com/psf/black\n    rev: 21.7b0\n    hooks:\n    -   id: black\n-   repo: https://gitlab.com/pycqa/flake8\n    rev: 3.7.9\n    hooks:\n    - id: flake8\n      additional_dependencies:\n          - flake8-unused-arguments\n</code></pre> <p><code>black</code> is a code autoformatter. It has opinions on what is good style and bad, and I mostly agree with those opinions. The very cool thing about <code>black</code> is that it does not just find instances where you are not following the style, it can automatically fix your code to follow the style.</p> <p><code>flake8</code> is a linter. It can check more kinds style errors, but it will not fix anything. It can only complain. This is mostly fine, because it is often trivial to fix the issues that <code>flake8</code> raises.</p> <p>Both of these tools needs some config to work as desired. See Full setup for details.</p>"},{"location":"2021-08-26-precommit/#optional-static-type-checking","title":"Optional static type checking","text":"<pre><code>-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v0.782\n    hooks:\n    -   id: mypy\n        args: [--ignore-missing-imports]\n</code></pre> <p>You can optionally do static typing in Python now. <code>mypy</code> is a tool to run static analysis on your python files and it will complain if you are inputting or return types that don't match your typehints.</p>"},{"location":"2021-08-26-precommit/#full-setup","title":"Full setup","text":"<p>If you just want to copy my setup, add these three files to the root of your repo:</p> .pre-commit-config.yaml<pre><code>repos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v3.2.0\n    hooks:\n    -   id: check-ast\n    -   id: check-json\n    -   id: check-yaml\n    -   id: debug-statements\n    -   id: detect-aws-credentials\n        args: [--allow-missing-credentials]\n    -   id: detect-private-key\n    -   id: check-merge-conflict\n    -   id: check-added-large-files\n        args: ['--maxkb=3000']\n-   repo: https://github.com/pre-commit/mirrors-mypy\n    rev: v0.782\n    hooks:\n    -   id: mypy\n        args: [--ignore-missing-imports]\n-   repo: https://github.com/pycqa/isort\n    rev: 5.8.0\n    hooks:\n    - id: isort\n      name: isort\n-   repo: https://github.com/psf/black\n    rev: 21.7b0\n    hooks:\n    -   id: black\n-   repo: https://gitlab.com/pycqa/flake8\n    rev: 3.7.9\n    hooks:\n    - id: flake8\n      additional_dependencies:\n          - flake8-unused-arguments\n-   repo: https://github.com/kynan/nbstripout\n    rev: 0.5.0\n    hooks:\n    - id: nbstripout\n</code></pre> pyproject.toml<pre><code>[tool.black]\nline-length = 100\ninclude = '\\.pyi?$'\nexclude = '''\n/(\n    \\.git\n  | \\.hg\n  | \\.mypy_cache\n  | \\.tox\n  | \\.venv\n  | _build\n  | buck-out\n  | build\n  | dist\n)/\n'''\n\n[tool.isort]\nprofile = \"black\"\nline_length = 100\n</code></pre> .flake8<pre><code>[flake8]\nignore = E203, E266, E501, W503\nmax-line-length = 100\nmax-complexity = 18\nselect = B,C,E,F,W,T4,B9,U100\nunused-arguments-ignore-abstract-functions = True\n</code></pre>"},{"location":"2021-08-26-precommit/#updates","title":"Updates","text":"<ul> <li>2021-09-08: Add <code>flake8-unused-arguments</code>.</li> </ul>"},{"location":"2023-02-26-azure-docker-cache/","title":"Caching Docker images in Azure DevOps Pipelines","text":"<p><code>TL;DR</code>: Go to the bottom of the post to see the full Pipeline template.</p>"},{"location":"2023-02-26-azure-docker-cache/#the-problem","title":"The problem","text":"<p>In the Data Science team at DFDS, we are using Azure DevOps Pipelines to build and deploy our models. We are using Docker containers to package our models, and we are using Azure Pipelines for our CI/CD.</p> <p>For most projects we will build the docker images in:</p> <ol> <li>The pull request: To make sure the docker image can be built and sometimes also to run some tests in the new container.</li> <li>After merging to main: To build the final image that will be deployed to production.</li> </ol> <p>Step 1 usually happens more than once, as issues with a PR will often require multiple iterations of reviews and fixes. For this reason, it is important that the build time is as short as possible. Long feedback loops are not good for productivity.</p> <p>So the solution is to cache the docker images between builds. Azure Pipelines even has a Cache task that claims to help with caching docker builds. But the commands listed on that documentation page have never worked for me.</p>"},{"location":"2023-02-26-azure-docker-cache/#the-solution","title":"The solution","text":"<p>My brilliant friend Morten Hels came up with a solution that works. I'm taking the liberty of writing it down here, but he is the one who deserves the credit.</p> <p>Instead of using <code>docker save</code> and <code>docker load</code> for (attempting to) make cached docker layers available, we use <code>docker buildx</code> to build the image from, and save to, a cache.</p> <p>The commend to run is:</p> <pre><code>docker buildx create --name builder --driver docker-container --use #1\ndocker buildx build \\                                               \n    --cache-from=type=local,src=docker_cache \\                      #2\n    --cache-to=type=local,dest=docker_cache,mode=max \\              #3\n    --file Dockerfile \\                                             \n    --output=type=docker,name=myimage \\                             #4\n    .\n</code></pre> <ol> <li>Create a new builder, and use it. This is needed to make the <code>--cache-from</code> and <code>--cache-to</code> options available. I'm using the <code>docker-container</code> driver, but there are other options available. This one is just the easiest to set up, both locally and in a pipeline.</li> <li>Use the local cache as a source for the build. This will make the build use the cached layers if they are available.</li> <li>Save the layers that were used in the build to the local cache. This will make the layers available for the next build.</li> <li>Set the output to be a docker image. This is needed to make the image available for the next step in the pipeline, e.g. pushing it to a registry.</li> </ol>"},{"location":"2023-02-26-azure-docker-cache/#the-pipeline-template","title":"The pipeline template","text":"<p>Here is a complete pipeline template that you can use in your own pipelines.</p> templates.yaml<pre><code>parameters:\n  - name: docker_image_name\n    type: string\n    displayName: 'The name of the Docker image to build. Example: klaur-testing.'\n  - name: additional_docker_build_args\n    type: string\n    default: ''\n    displayName: 'Additional arguments to pass to the docker build command. Example: --build-arg SOME_ARG=some_value.'\n  - name: dockerfile_path\n    type: string\n    default: 'Dockerfile'\n    displayName: 'The path to the Dockerfile to use. Example: Dockerfile.'\n  - name: docker_build_context\n    type: string\n    default: '.'\n    displayName: 'The path to the directory to use as the build context. Example: .'\n\nsteps:\n  - task: Cache@2\n    displayName: Cache Docker layers\n    inputs:\n      key: '\"docker\" | \"$(Agent.OS)\" | \"${{ parameters.docker_image_name }}\" | ${{ parameters.dockerfile_path }}'\n      restoreKeys: |\n        \"docker\" | \"$(Agent.OS)\" | \"${{ parameters.docker_image_name }}\"\n      path: $(Pipeline.Workspace)/docker_cache\n\n  - script: |\n      docker buildx create --name builder --driver docker-container --use\n      docker buildx build \\\n        --cache-from=type=local,src=$(Pipeline.Workspace)/docker_cache \\\n        --cache-to=type=local,dest=$(Pipeline.Workspace)/docker_cache,mode=max \\\n        --file ${{ parameters.dockerfile_path }} \\\n        --output=type=docker,name=${{ parameters.docker_image_name }} \\\n        ${{ parameters.additional_docker_build_args }} ${{ parameters.docker_build_context }}\n    displayName: Build Docker image\n    env:\n      DOCKER_BUILDKIT: 1\n</code></pre> <p>If the above yaml is saved in a <code>templates.yaml</code> file, you can use it in your pipeline like this:</p> azure-pipelines.yml<pre><code>jobs:\n  - job: BuildDockerImage\n    steps:\n      - template: templates.yaml\n        parameters:\n          docker_image_name: 'my-image'\n          additional_docker_build_args: '--build-arg SOME_ARG=some_value'\n          dockerfile_path: 'Dockerfile'\n          docker_build_context: '.'\n</code></pre>"},{"location":"2023-02-26-azure-docker-cache/#references","title":"References","text":"<ul> <li>Morten Hels - Great data scientist moonlighting as an excellent data engineer.</li> <li>Stack Overflow post that Morten claims got him on the right track.</li> <li>Docker documentation on <code>docker buildx</code>.</li> </ul>"},{"location":"2024-10-28-docker-build-remote/","title":"Build docker images on remote Linux VM","text":"<p><code>TL;DR</code>: Create a Linux VM in the cloud, then create a docker context for it with</p> <pre><code>docker context create linux-builder --docker \"host=ssh://username@remote-ip\"\n</code></pre> <p>then build your image with</p> <pre><code>docker buildx build --context linux-builder --platform linux/amd64 -t my-image .\n</code></pre>"},{"location":"2024-10-28-docker-build-remote/#problem-building-some-docker-images-on-a-modern-mac-fails","title":"Problem: Building some Docker images on a modern Mac fails","text":"<p>At work, I'm using an M3 Macbook. It's a great machine, but it's not perfect. One issue is that I can't always build Docker images target to <code>linux/amd64</code> on it.</p> <p>Recently, I had an issue where I needed to package a Python application in Docker, and one of the dependencies was <code>pytorch</code>. I suspect that is where my issue was coming from.</p> <p>Building the image on Mac works fine when running it on the same machine, but when I try to run it on a Linux machine, it fails with the following error:</p> <pre><code>exec /app/.venv/bin/python: exec format error\n</code></pre> <p>This indicated that the Python binary was built for the wrong architecture. Luckily, you can specify the target architecture using the <code>--platform</code> flag when building the image.</p> <pre><code>docker buildx build --platform linux/amd64 -t my-image .\n</code></pre> <p>Unfortunately, this didn't work for me. I suspect that the <code>pytorch</code> dependency was causing the issue. I got the following error:</p> <pre><code>Cannot install nvidia-cublas-cu12.\n</code></pre>"},{"location":"2024-10-28-docker-build-remote/#solution-build-the-image-on-a-remote-linux-vm","title":"Solution: Build the image on a remote Linux VM","text":"<p>To solve this issue, I decided to build the image on a remote x86_64 Linux VM. This way, I can ensure that the image is built for the correct architecture.</p> <p>I used an Azure Virtual Machine with an Ubuntu 24.04 image. I enabled \"Auto-shutdown\" at midnight every day to save costs.</p> <p>After ssh-ing into the VM, I installed docker and ensured the user was added to the docker group.</p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\nsudo usermod -aG docker azureuser\n</code></pre> <p>Check that the docker daemon is running:</p> <pre><code>sudo systemctl status docker\n</code></pre> <p>Now, back on my local machine, I created a docker context for the remote VM:</p> <pre><code>docker context create linux-builder --docker \"host=ssh://azureuser@remote-ip\"\n</code></pre> <p>Now, I can build the image using the context:</p> <pre><code>docker buildx build --context linux-builder --platform linux/amd64 -t my-image .\n</code></pre> <p>I can also enable the context for all future commands:</p> <pre><code>docker context use linux-builder\n</code></pre>"},{"location":"2024-11-13-docker-kafka-test/","title":"Test kafka clients with Docker","text":"<p><code>TL;DR</code>: Use <code>pytest-docker</code> to create a test fixture that starts a Kafka container.</p>"},{"location":"2024-11-13-docker-kafka-test/#problem-i-want-to-test-my-kafka-client-but-i-dont-have-a-kafka-cluster","title":"Problem: I want to test my Kafka client, but I don't have a Kafka cluster","text":"<p>At work, we need to consume and produce messages to some queue. And one of the tools available already is Kafka.</p> <p>Before integrating with the existing Kafka cluster, I want to test my client code. I want to ensure that it can consume and produce messages correctly.</p> <p>I have an existing <code>BaseQueueService</code> class like this:</p> <pre><code>class BaseQueueService(ABC):\n    @abstractmethod\n    def publish(self, message: str) -&gt; None:\n        pass\n\n    @abstractmethod\n    def consume(self) -&gt; str | None:\n        pass\n</code></pre> <p>with existing implementations for Azure Service Bus and an InMemoryQueue for testing business logic.</p> <p>So I want to create a <code>KafkaQueueService</code> class that implements this interface. And I want to test it, but I don't have a Kafka cluster available.</p>"},{"location":"2024-11-13-docker-kafka-test/#solution-use-docker-to-start-a-kafka-container-for-testing","title":"Solution: Use docker to start a Kafka container for testing","text":"<p>I can use <code>pytest-docker</code> to create a test fixture that starts a Kafka container. This way, I can test my <code>KafkaQueueService</code> class without needing a Kafka cluster.</p> <p>This is how I did it:</p> <p>A <code>docker-compose.yml</code> file to start a Kafka container:</p> docker-compose.yml<pre><code>services:\n  zookeeper:\n    image: 'confluentinc/cp-zookeeper:latest'\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: 'confluentinc/cp-kafka:latest'\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n    ports:\n      - \"9092:9092\"\n    expose:\n      - \"29092\"\n\n  init-kafka:\n    image: 'confluentinc/cp-kafka:latest'\n    depends_on:\n      - kafka\n    entrypoint: [ '/bin/sh', '-c' ]\n    command: |\n      \"\n      # blocks until kafka is reachable\n      kafka-topics --bootstrap-server kafka:29092 --list\n\n      echo -e 'Creating kafka topics'\n      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic testtopic --replication-factor 1 --partitions 1\n      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic input_test_topic --replication-factor 1 --partitions 1\n      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic output_test_topic --replication-factor 1 --partitions 1\n\n      echo -e 'Successfully created the following topics:'\n      kafka-topics --bootstrap-server kafka:29092 --list\n      \"\n</code></pre> <p>A <code>conftest.py</code> file to create a test fixture that starts the Kafka container:</p> <pre><code>def check_kafka_ready(required_topics, host=\"localhost\", port=9092):\n    from confluent_kafka import KafkaException\n    from confluent_kafka.admin import AdminClient\n\n    try:\n        admin = AdminClient({\"bootstrap.servers\": f\"{host}:{port}\"})\n        topics = admin.list_topics(timeout=5)\n        # Check if all required topics are present\n        if all(topic in topics.topics for topic in required_topics):\n            return True\n        else:\n            return False\n    except KafkaException:\n        return False\n\n\n@pytest.fixture(scope=\"session\")\ndef kafka_url(docker_services):\n    \"\"\"Start kafka service and return the url.\"\"\"\n    port = docker_services.port_for(\"kafka\", 9092)\n    required_topics = [\"testtopic\", \"input_test_topic\", \"output_test_topic\"]\n    docker_services.wait_until_responsive(\n        check=lambda: check_kafka_ready(port=port, required_topics=required_topics),\n        timeout=30.0,\n        pause=0.1,\n    )\n    return f\"localhost:{port}\"\n</code></pre> <p>And finally, a test file to test the <code>KafkaQueueService</code> class:</p> <pre><code>@pytest.mark.kafka\ndef test_kafka_queue_can_publish_and_consume(kafka_url):\n    kafka_queue_service = KafkaQueueService(\n        broker=kafka_url,\n        topic=\"testtopic\",\n        group_id=\"testgroup\",\n    )\n    clear_messages_from_queue(kafka_queue_service)\n\n    unique_message = \"hello\" + str(uuid.uuid4())\n    kafka_queue_service.publish(unique_message)\n\n    received_message = kafka_queue_service.consume()\n    assert received_message == unique_message\n</code></pre> <p>Now I can test my <code>KafkaQueueService</code> class without needing a Kafka cluster. This even works on my CI/CD pipeline in Azure DevOps.</p> <p>NOTE: The <code>docker-services</code> fixture starts ALL the docker services in the <code>docker-compose.yml</code> file.</p>"},{"location":"2024-11-13-docker-kafka-test/#bonus-the-passing-implementation-of-kafkaqueueservice","title":"Bonus: The passing implementation of <code>KafkaQueueService</code>","text":"<p>This passes the test above (and a few other tests I wrote):</p> <pre><code>from confluent_kafka import Consumer, KafkaError, Producer\n\nclass KafkaQueueService(BaseQueueService):\n    def __init__(self, broker: str, topic: str, group_id: str):\n        # Configuration for the producer and consumer\n        self.topic = topic\n        self.producer: Producer = Producer({\"bootstrap.servers\": broker})\n        self.consumer: Consumer = Consumer(\n            {\n                \"bootstrap.servers\": broker,\n                \"group.id\": group_id,\n                \"auto.offset.reset\": \"earliest\",\n                \"enable.partition.eof\": \"true\",\n            }\n        )\n        self.consumer.subscribe([self.topic])\n\n    def publish(self, message: str) -&gt; None:\n        \"\"\"Publish a message to the Kafka topic.\"\"\"\n        logger.debug(f\"Publishing message to topic {self.topic}: {message}\")\n\n        self.producer.produce(self.topic, message.encode(\"utf-8\"))\n        self.producer.flush()\n\n    def consume(self) -&gt; str | None:\n        \"\"\"Consume a single message from the Kafka topic.\"\"\"\n        logger.debug(f\"Consuming message from topic {self.topic}\")\n\n        # Get the next message\n        message = self.consumer.poll(timeout=20)\n        if message is None:\n            logger.debug(\"Consumer poll timeout\")\n            return None\n        # No new message\n        if message.error() is not None and message.error().code() == KafkaError._PARTITION_EOF:\n            logger.debug(\"No new messages in topic\")\n            return None\n        # Check for errors\n        if message.error() is not None:\n            raise Exception(f\"Consumer error: {message.error()}\")\n        self.consumer.commit(message, asynchronous=False)\n        return message.value().decode(\"utf-8\")\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(topic={self.topic})\"\n</code></pre>"},{"location":"2024-11-15-databricks-cli-auth/","title":"Use databricks profiles to emulate service principals","text":"<p><code>TL;DR</code>: Edit your <code>~/.databrickscfg</code> file to create a profile for your service principals.</p>"},{"location":"2024-11-15-databricks-cli-auth/#problem-the-feedback-loop-when-developing-a-cicd-pipeline-is-too-slow","title":"Problem: The feedback loop when developing a CI/CD pipeline is too slow","text":"<p>I have a CI/CD pipeline that interacts with a Databricks workspace through the Databricks CLI. I usually develop the pipeline locally, testing it against a sandbox Databricks workspace, authenticated as myself.</p> <p>But when I deploy the pipeline to the CI/CD environment, it runs as a service principal, first against a dev workspace, then against a prod workspace.</p> <p>There can be some issues that only appear when running as a service principal, like permissions errors or workspace configurations. And the feedback loop is too slow: I have to commit, push, wait for the pipeline to run, check the logs, and repeat.</p> <p>I want to test the pipeline locally, authenticated as a service principal, to catch these issues earlier.</p>"},{"location":"2024-11-15-databricks-cli-auth/#solution-use-databricks-profiles-to-emulate-service-principals","title":"Solution: Use databricks profiles to emulate service principals","text":"<p>Reading about the one million ways to authenticate to an Azure Databricks workspace is enough to give me a headache (Seriously, there are too many options). I have previously used environment variables to authenticate as a service principal, the various secrets in an <code>.env</code> file, and commenting and un-commenting as needed. It is a mess, and I'm guaranteed to forget to switch back to my user account at some point.</p> <p>Instead, I can use databricks profiles to store the different authentication configurations. In <code>~/.databrickscfg</code>, I can create a profile for each service principal, and switch between them with the <code>--profile</code> flag.</p> <p>Here is an example of a <code>~/.databrickscfg</code> file with two Service principal profiles:</p> .databrickscfg<pre><code>[DEFAULT]\nhost  = &lt;SOME_HOST&gt;\ntoken = &lt;SOME_TOKEN&gt;\n\n[project-prod-sp]\nhost                = \nazure_client_id     = \nazure_client_secret = \nazure_tenant_id     = \n\n[project-dev-sp]\n&lt;same setup as above&gt;\n</code></pre> <p>Of course, you should replace the placeholders with the actual values.</p> <p>To test what workspace and user your profile is using, you can try the following command:</p> <pre><code>databricks auth describe --profile project-prod-sp\n</code></pre> <p>This will also show you where the authentication is coming from (because, as I mentioned above, there are too many ways to authenticate).</p> <p>Finally, you can run your pipeline locally, using the <code>--profile</code> flag to specify that you want to use the service principal profile:</p> <pre><code>databricks bundle deploy --profile project-dev-sp\n</code></pre>"},{"location":"2024-11-15-databricks-cli-auth/#alternative-to-using-profile-flag","title":"Alternative to using <code>--profile</code> flag","text":"<p>If you still want to use environment variables, you can set the <code>DATABRICKS_CONFIG_PROFILE</code> variable to the profile name you want to use, e.g.:</p> <pre><code>DATABRICKS_CONFIG_PROFILE=DEFAULT\n</code></pre>"},{"location":"2025-01-17-migrate-mkdocs/","title":"Migrating static site from Jekyll to MkDocs","text":""},{"location":"2025-01-17-migrate-mkdocs/#intro","title":"Intro","text":"<p>I have posted (very) few post to my blog over the years. Recently, one of the things holding me back from posting is that I can't really build it locally any more. I'm using Github Pages, and although it probably very easy to use, I just use it so rarely that I don't really know what is going on.</p> <p>My brief guide says to run <code>bundle install</code>, but I don't have <code>bundle</code> installed. I also don't know what it is. Github tells me to install Jekyll and Ruby. I don't have either of them.</p> <p>At work I use Python a lot, and I have created a few docs sites with MkDocs, with Material for MkDocs helping out in making everything pretty. I want to use that tool-stack instead. All the content is markdown anyway, so it should not be too bad.</p>"},{"location":"2025-01-17-migrate-mkdocs/#build-locally","title":"Build locally","text":"<p>I start by cloning https://github.com/KPLauritzen/kplauritzen.github.io and opening it VSCode.</p> <p>Let's create a justfile to document how to interact with the repo.</p> justfile<pre><code>install:\n  uv sync --all-extras\n  uv run pre-commit install\n\nlint:\n  uv run pre-commit run --all-files\n</code></pre> <p>None of this works yet, there is no Python project, but it is a start.</p> <p>I set up <code>pyproject.toml</code> with <code>uv init</code> and add some packages with <code>uv add pre-commit mkdocs mkdocs-material</code>.</p> <p>Now I just need the most basic config for MkDocs and we are ready to serve some HTML!</p> mkdocs.yml<pre><code>site_name: KPLauritzen.dk\ndocs_dir: _posts\n</code></pre> <p>I can see my site locally with <code>mkdocs serve</code></p> <p>It's terrible, but it works! I add that as a command in the <code>justfile</code> </p>"},{"location":"2025-01-17-migrate-mkdocs/#slightly-prettier","title":"Slightly prettier","text":"<p>How little effort can I put in to make this tolerable?</p> <ul> <li> <p>Add a theme to <code>mkdocs.yml</code></p> <pre><code>  theme: material\n</code></pre> </li> <li> <p>Move <code>index.md</code> to <code>posts_</code> so we don't start with a 404 error.</p> </li> </ul> <p>That's it, now it actually looks serviceable. </p> <p>There is A BUNCH of improvements that could be helpful, but it is too much fun to do. I will save some of that for a rainy day.</p> <p>For now, I will just try to create a github workflow to publish this.</p>"},{"location":"2025-01-17-migrate-mkdocs/#publish-again-to-github-pages","title":"Publish again to Github Pages","text":"<p>I already have a Github Action called <code>jekyll.yml</code>. Let's delete that and make a new one.</p> <p>I start by stealing the basic outline from mkdocs-material. After that, I follow the guide to uv in Github Actions.</p> <p>This is the result:</p> .github/workflows/ci.yml<pre><code>name: ci \non:\n  push:\n    branches:\n      - master \n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - name: Install uv\n        uses: astral-sh/setup-uv@v5\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV \n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: uv run mkdocs gh-deploy --force\n</code></pre> <p>EDIT: After publishing I had some problems with my custom domain, <code>kplauritzen.dk</code>. Every time I ran <code>mkdocs gh-deploy</code> it wanted to deploy to <code>kplauritzen.github.io</code> instead.</p> <p>I think the solution is to create a <code>CNAME</code> file in <code>_posts/</code> as that will get picked up during the build. See the docs.</p>"},{"location":"2025-01-18-homelab-1/","title":"Homelab","text":"","tags":["homelab"]},{"location":"2025-01-18-homelab-1/#starting-the-homelab","title":"Starting the homelab","text":"<p>I found an old mini PC (ASUS Mini PC PN30), left in a drawer from when I thought I needed it to run a Plex media server. With a sudden (unexpected) burst of motivation I decided to run a local kubernetes cluster on it. (In hindsight, I think I might also have been inspired to try self-hosting an RSS reader by this post. I just got distracted, by deciding to self-host using kubernetes).</p>","tags":["homelab"]},{"location":"2025-01-18-homelab-1/#making-a-plan","title":"Making a plan","text":"<p>I asked ChatGPT and Claude for help on how to set up a simple kubernetes setup at home. After some back-and-forth I landed on installing Debian with no graphical desktop environment, and then installing <code>k3s</code>. The choice of <code>k3s</code> was mainly made to limit the resource requirements. The Mini PC is not exactly beefy, with an underpowered CPU and only 4GB RAM (While trying to confirm this number, I found the listing for this on Amazon and it claims that I can upgrade to 8GB RAM. I might do that at some point).</p>","tags":["homelab"]},{"location":"2025-01-18-homelab-1/#install-debian","title":"Install Debian","text":"<p>I downloaded an ISO of Debian 12 and made a bootable usb. I connected the Mini PC to a monitor, keyboard and mouse and booted the Debian installer from the usb stick. I selected graphical installer and followed the guidance. I did not create a root user, instead letting my own user get sudo privileges. I did not install a desktop environment. I gave it the hostname <code>tyr</code>. I made sure to select SSH, to allow access after I unplug the peripherals.</p>","tags":["homelab"]},{"location":"2025-01-18-homelab-1/#install-tools","title":"Install tools","text":"<pre><code>sudo apt-get update &amp;&amp; sudo apt-get upgrade\nsudo apt-get install -y vim git curl wget htop\n</code></pre> <p>I tried accessing the mini PC over shh from my desktop.</p> <pre><code>ssh kasper@tyr.local\n</code></pre> <p>This did not work, but using the local IP directly works fine.</p> <p>I really want to use the <code>hostname.local</code> thing, I learned that it is called mDNS, and I need a mDNS service. I installed Avahi, both on my desktop and on the Mini PC</p> <pre><code>sudo apt-get install avahi-daemon\n</code></pre>","tags":["homelab"]},{"location":"2025-01-18-homelab-1/#install-k3s","title":"Install <code>k3s</code>","text":"<p>Now, to install <code>k3s</code>. Following docs at https://k3s.io/.</p> <pre><code>curl -sfL https://get.k3s.io | sh - \n</code></pre> <p>After a minute, the kubernetes cluster is running and I can query it from <code>tyr</code></p> <pre><code>$ sudo k3s kubectl get node \nNAME   STATUS   ROLES                  AGE   VERSION\ntyr    Ready    control-plane,master   15h   v1.31.4+k3s1\n</code></pre> <p>Next, I want to access it from my desktop. Following the k3s guide I copy <code>/etc/rancher/k3s/k3s.yaml</code> from <code>tyr</code> to <code>~/.kube/config</code> my desktop with <code>scp</code>, and edit the <code>server</code> field to point to IP of <code>tyr</code>. I tried a lot get <code>tyr.local</code> to resolve instead of the IP, but as far as I can tell, <code>kubectl</code> is not using the mDNS stuff from above. Here is the last chat message (in a long back-and-forth) from <code>o1</code> on why <code>.local</code> does not work.</p> <p>A statically compiled binary often does not use the system's usual NSS (Name Service Switch) mechanisms\u2014like <code>/etc/nsswitch.conf</code> and <code>libnss-mdns</code>\u2014for hostname resolution. Instead, it typically performs \"pure DNS\" lookups.</p> <p>That explains why:</p> <ul> <li>ping tyr.local succeeds, because it honors <code>nsswitch.conf</code> and uses Avahi/mDNS.</li> <li>kubectl fails on <code>tyr.local</code>, because it bypasses your local mDNS setup and tries querying a DNS server that doesn't know about <code>.local</code> names.'</li> </ul> <p>ChatGPT suggest some ways to fix it, but the simplest seemed to be to just plug in the IP.</p> <p>I made sure to go to my router and reserve the local IP address of <code>tyr</code>, so it does not change after a reboot or something.</p> <p>And finally, I can run the following from my desktop</p> <pre><code>$ kubectl get node \nNAME   STATUS   ROLES                  AGE   VERSION\ntyr    Ready    control-plane,master   44h   v1.31.4+k3s1\n</code></pre>","tags":["homelab"]},{"location":"2025-01-24-homelab-2/","title":"Homelab: Commiting my secrets to git","text":"<p>I spent some time tonight setting configuring some services on my home kubernetes cluster. See this post for more details on how I set up the cluster. So far it's been a fun experiment to see if I can avoid anything spontanously catching file. At work there is a full team of experts dedicated to keep our cluster running smoothly. At home, there is... me. </p> <p>Today I managed to get FreshRSS and Atuin Sync running.</p> <p>I've been using Cursor as a guide generating the yaml files and asking questions about how Kubernetes works. I think I am a decent user of Kubernetes clusters, but a rank novice as an operator of a cluster. </p>","tags":["homelab"]},{"location":"2025-01-24-homelab-2/#freshrss","title":"FreshRSS","text":"<p>I want to try to get away from doomscrolling, and being caught in some algorithmically generated news feed. I'll try FreshRSS for a while at least. </p> <p>To get started I asked Cursor to generate a deployment, giving it a link to the FreshRSS documentation.</p> <p>I had to go back and forth a few times to understand how to get a URL to resolve on my home network.  The kubernetes cluster is running on the host <code>tyr</code>, so I can ping that from my home network on <code>tyr.local</code>. </p> <p>Initially I wanted to host FreshRSS at <code>rss.tyr.local</code>, but I didn't figure out how to do that. Instead I hosted it at <code>tyr.local/rss</code> and then added Middleware to strip the <code>/rss</code> path before sending the traffic to the Service. </p> Complete manifest <pre><code>---\n# deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: freshrss\nnamespace: freshrss\nlabels:\n    app: freshrss\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: freshrss\ntemplate:\n    metadata:\n    labels:\n        app: freshrss\n    spec:\n    containers:\n        - name: freshrss\n        image: freshrss/freshrss:latest\n        ports:\n            - containerPort: 80\n        env:\n            - name: TZ\n            value: \"Europe/Copenhagen\"\n            - name: CRON_MIN\n            value: \"13,43\"\n        volumeMounts:\n            - name: data\n            mountPath: /var/www/FreshRSS/data\n            - name: extensions\n            mountPath: /var/www/FreshRSS/extensions\n        resources:\n            requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n            limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n    volumes:\n        - name: data\n        persistentVolumeClaim:\n            claimName: freshrss-data\n        - name: extensions\n        persistentVolumeClaim:\n            claimName: freshrss-extensions\n\n---\n# ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: freshrss\nnamespace: freshrss\nannotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.middlewares: \"freshrss-strip-prefix@kubernetescrd\"\n    traefik.ingress.kubernetes.io/router.tls: \"false\"\nspec:\nrules:\n    - host: tyr.local\n    http:\n        paths:\n        - path: /rss\n            pathType: Prefix\n            backend:\n            service:\n                name: freshrss\n                port:\n                number: 80\n\n---\n# middleware.yaml\n\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\nname: strip-prefix\nnamespace: freshrss\nspec:\nstripPrefix:\n    prefixes:\n    - /rss\n    forceSlash: true\n\n---\n# namespace.yaml\n\napiVersion: v1\nkind: Namespace\nmetadata:\nname: freshrss\n\n---\n# pvc.yaml\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: freshrss-data\nnamespace: freshrss\nspec:\naccessModes:\n    - ReadWriteOnce\nresources:\n    requests:\n    storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: freshrss-extensions\nnamespace: freshrss\nspec:\naccessModes:\n    - ReadWriteOnce\nresources:\n    requests:\n    storage: 100Mi\n\n---\n# service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\nname: freshrss\nnamespace: freshrss\nspec:\nselector:\n    app: freshrss\nports:\n    - port: 80\n    targetPort: 80\ntype: ClusterIP\n</code></pre>","tags":["homelab"]},{"location":"2025-01-24-homelab-2/#atuin","title":"Atuin","text":"<p>Again, I just asked Cursor to generate the Kubernetes manifests after linking to the self-hosting docs. </p> <p>One issue I had here, was that I want to commit the manifests to git, but Atuin requires a password to communicate with the local Postgres database. </p> secrets.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: atuin-secrets\ntype: Opaque\nstringData:\n  ATUIN_DB_USERNAME: atuin\n  ATUIN_DB_PASSWORD: seriously-insecure\n  ATUIN_HOST: \"127.0.0.1\"\n  ATUIN_PORT: \"8888\"\n  ATUIN_OPEN_REGISTRATION: \"true\"\n  ATUIN_DB_URI: \"postgres://atuin:seriously-insecure@postgres/atuin\"\nimmutable: true\n</code></pre> <p>Since both services are local and not exposed to the internet, there is probably no harm in commiting something like this. </p> <p>But I had a discussion about this problem with my smart, very kubernetes savvy friend, Lasse. So I wanted to find some solution to this that did not commit any deadly sins, such as committing a secret in plaintext. </p> <p>Again, I asked Cursor for suggestions, prompting it to not make any changes to my files, just list out options. </p> <p>Here is the full list of suggestions:</p> List some ways I can add secrets here without leaking it in git <ol> <li> <p>Using Sealed Secrets:</p> <pre><code># Install sealed-secrets controller\nhelm install sealed-secrets sealed-secrets/sealed-secrets\n# Encrypt your secrets\nkubeseal &lt; atuin/secrets.yaml &gt; atuin/sealed-secrets.yaml\n</code></pre> <p>This lets you safely commit encrypted secrets that only your cluster can decrypt.</p> </li> <li> <p>Using External Secrets Operator with a secrets manager:</p> <pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\nname: atuin-secrets\nnamespace: atuin\nspec:\nrefreshInterval: 1h\nsecretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\ntarget:\n    name: atuin-secrets\ndata:\n    - secretKey: ATUIN_DB_PASSWORD\n    remoteRef:\n        key: atuin/db-password\n</code></pre> <p>This fetches secrets from services like HashiCorp Vault, AWS Secrets Manager, or Azure Key Vault.</p> </li> <li> <p>Using Kustomize with secret generators:     <pre><code># kustomization.yaml\nsecretGenerator:\n- name: atuin-secrets\nnamespace: atuin\nenvs:\n- .env # This file is not committed\n</code></pre>     This generates secrets from local files that aren't committed.</p> </li> <li> <p>Using environment variables with a CI/CD pipeline:     <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: atuin-secrets\nnamespace: atuin\nstringData:\nATUIN_DB_PASSWORD: ${ATUIN_DB_PASSWORD}\nATUIN_DB_URI: \"postgres://atuin:${ATUIN_DB_PASSWORD}@postgres/atuin\"\n</code></pre>     Your CI/CD system (like GitHub Actions) replaces the variables before applying.</p> </li> <li> <p>Using SOPS (Secrets OPerationS):     <pre><code># Encrypt secrets\nsops -e -i atuin/secrets.yaml\n\n# Decrypt when needed\nsops -d atuin/secrets.yaml | kubectl apply -f -\n</code></pre>     This encrypts secrets with age, PGP, or cloud KMS keys.</p> </li> </ol> <p>I chose to use Sealed Secrets. It sounded by far the simplest, and I could sort-of understand what was going on:</p> <ul> <li>I install the SealedSecret helm chart on my cluster. This can encrypt and decrypt text using some fancy crypto magic. Basically a private/public key pair is generated and stored as Kubernetes secerets and these are used to encrypt my secrets.</li> <li>I install <code>kubeseal</code> on my local machine. It can communicate with the SealedSecret controller running in the cluster. </li> <li>I pass a local (uncommitted) kubernetes <code>Secret</code> to <code>kubeseal</code>, it encrypts it and I get back a <code>SealedSecret</code>. </li> <li>I can then store, apply and commit this <code>SealedSecret</code>. It will get unsealed when applied to my cluster (so my services can use it), but the unsealing only happens inside the cluster. My local manifest file is encrypted. </li> </ul> <p>Let's say I want to encrypt this Secret secrets.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: atuin-secrets\n  namespace: atuin\ntype: Opaque\nstringData:\n  ATUIN_DB_USERNAME: atuin\n  ATUIN_DB_PASSWORD: \"123\"\n  ATUIN_DB_URI: \"postgres://username:123@postgres/atuin\" # Match the password here\n</code></pre></p> <p>I can run <code>kubeseal</code> to encrypt: <pre><code>kubeseal &lt; secrets.yaml &gt; sealed-secrets.yaml\n</code></pre></p> <p>and I get back</p> sealed-secrets.yaml<pre><code>apiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\n  creationTimestamp: null\n  name: atuin-secrets\n  namespace: atuin\nspec:\n  encryptedData:\n    ATUIN_DB_PASSWORD: AgBKfphBarMiNX8CIsvjAXqEtRp/Bq+a4y67k/M6bxMm1w/[TRUNCATED FOR SPACE]\n    ATUIN_DB_URI: AgCfm2AisGVBlMrOqPvMWOor0e0UXDruZnWVG3klrfSzbtZfrzYF4x[TRUNCATED FOR SPACE]\n    ATUIN_DB_USERNAME: AgAt8yDkKRjmvJtB4ecxOOcuEm1Zcoa8pX1UvtvwAAT4M18PN3JK[TRUNCATED FOR SPACE]\n  template:\n    metadata:\n      creationTimestamp: null\n      name: atuin-secrets\n      namespace: atuin\n    type: Opaque\n</code></pre> <p>Pretty cool!  I have also backed up the Sealed Secrets private key in my 1Password. </p> <pre><code>kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml &gt; sealed-secrets-master.key\n</code></pre> <p>If my cluster suddenly catches fire, I can recreate my deployments in a new cluster by adding the key to that cluster <pre><code>kubectl apply -f sealed-secrets-master.key\nkubectl delete pod -n kube-system -l name=sealed-secrets-controller\n</code></pre></p> <p>Here is the complete manifest</p> Complete manifest <pre><code># config.yaml\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: atuin-config\nnamespace: atuin\ndata:\nATUIN_HOST: \"0.0.0.0\"\nATUIN_PORT: \"8888\"\nATUIN_OPEN_REGISTRATION: \"true\"\n\n---\n# deployment.yaml\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: postgres\nnamespace: atuin\nspec:\nreplicas: 1\nstrategy:\n    type: Recreate # Prevent data corruption by ensuring only one pod runs\nselector:\n    matchLabels:\n    app: postgres\ntemplate:\n    metadata:\n    labels:\n        app: postgres\n    spec:\n    containers:\n        - name: postgresql\n        image: postgres:14\n        ports:\n            - containerPort: 5432\n        env:\n            - name: POSTGRES_DB\n            value: atuin\n            - name: POSTGRES_PASSWORD\n            valueFrom:\n                secretKeyRef:\n                name: atuin-secrets\n                key: ATUIN_DB_PASSWORD\n            - name: POSTGRES_USER\n            valueFrom:\n                secretKeyRef:\n                name: atuin-secrets\n                key: ATUIN_DB_USERNAME\n        lifecycle:\n            preStop:\n            exec:\n                command:\n                [\n                    \"/usr/local/bin/pg_ctl\",\n                    \"stop\",\n                    \"-D\",\n                    \"/var/lib/postgresql/data\",\n                    \"-w\",\n                    \"-t\",\n                    \"60\",\n                    \"-m\",\n                    \"fast\",\n                ]\n        resources:\n            requests:\n            cpu: 100m\n            memory: 100Mi\n            limits:\n            cpu: 250m\n            memory: 600Mi\n        volumeMounts:\n            - mountPath: /var/lib/postgresql/data/\n            name: database\n    volumes:\n        - name: database\n        persistentVolumeClaim:\n            claimName: database\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: atuin\nnamespace: atuin\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: atuin\ntemplate:\n    metadata:\n    labels:\n        app: atuin\n    spec:\n    containers:\n        - name: atuin\n        image: ghcr.io/atuinsh/atuin:v18.4.0 # Using a specific version as recommended\n        args:\n            - server\n            - start\n        env:\n            - name: ATUIN_DB_URI\n            valueFrom:\n                secretKeyRef:\n                name: atuin-secrets\n                key: ATUIN_DB_URI\n            - name: ATUIN_HOST\n            valueFrom:\n                configMapKeyRef:\n                name: atuin-config\n                key: ATUIN_HOST\n            - name: ATUIN_PORT\n            valueFrom:\n                configMapKeyRef:\n                name: atuin-config\n                key: ATUIN_PORT\n            - name: ATUIN_OPEN_REGISTRATION\n            valueFrom:\n                configMapKeyRef:\n                name: atuin-config\n                key: ATUIN_OPEN_REGISTRATION\n        ports:\n            - containerPort: 8888\n        resources:\n            limits:\n            cpu: 250m\n            memory: 1Gi\n            requests:\n            cpu: 250m\n            memory: 1Gi\n        volumeMounts:\n            - mountPath: /config\n            name: atuin-config\n    volumes:\n        - name: atuin-config\n        persistentVolumeClaim:\n            claimName: atuin-config\n\n---\n# ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: atuin\nnamespace: atuin\nannotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n    traefik.ingress.kubernetes.io/router.middlewares: \"atuin-strip-prefix@kubernetescrd\"\nspec:\nrules:\n    - host: tyr.local\n    http:\n        paths:\n        - path: /atuin\n            pathType: Prefix\n            backend:\n            service:\n                name: atuin\n                port:\n                number: 8888\n\n---\n# middleware.yaml\n\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\nname: strip-prefix\nnamespace: atuin\nspec:\nstripPrefix:\n    prefixes:\n    - /atuin\n    forceSlash: true\n\n---\n# namespace.yaml\n\napiVersion: v1\nkind: Namespace\nmetadata:\nname: atuin\n\n---\n# sealed-secrets.yaml\n\n---\napiVersion: bitnami.com/v1alpha1\nkind: SealedSecret\nmetadata:\ncreationTimestamp: null\nname: atuin-secrets\nnamespace: atuin\nspec:\nencryptedData:\n    ATUIN_DB_PASSWORD: AgBKfphBarMiNX8CIsvjAXqEtRp/Bq+a4y67k/M6bxMm1w/fJUERNqBKaPWqaABfHR4WEk9ePj4CWcVbHb2xVCviX4zYE4pZ9onMvzRGJa2UUl1qRsJGN/ooMRJux+ztfSXJfRzzZxt1QjBlJOmMxG0XjKu0TdahXnI4BMJ2rrBPPmWx9sr4z8YxG8BU/TL8DiJGiD2DtarQWmqSogueGpsOE/9hdeWvW4E7RNlcd7JJ0Hv/nELlhVIUB9fzGoaioDJO6qodYBWNtt2ckyNp3KwoOKXddwRV5tq1ggPKnZOqlHpDgmTaYAFNPXVGIpMNxzUfs+CU0VdT60hx5e3qMbVD86NrnqmbQ38GYc/A7TDrWImSEPjkweLPSTgK5YuQEHJBGYDy9jNNVTMHwfcXkAZkD8swu8+2Whw6No1D2WO2LwewVdTDOynjVhekGk3UF6B2lqIn9TowkIBbZZ6mYYK4VzXRCRXmo2ZiEqDMQK78ejUHdK5m43cZ9M+BEmE3lKzAmgZt+xons/xcisI63pff31urXWZsFylZvnVUnR/l0cp5jmr8KDnMp1WDPf+UyhSlxVvnfAKRyXIGi6jpMQluXVvx/waX4MdqgJMfyn3cQ6tFH4YiZCX6kdNNWjJp5lYxmhRdqWRznCB1vxuWIfXCc9eUT8Kz0Houmw/S8HR11ApNoxopbalC23wdTa9ZXlJdC4bXElfdC8HHwjTcNezDN9mc+4e+WdaKkbuYZljP\n    ATUIN_DB_URI: AgCfm2AisGVBlMrOqPvMWOor0e0UXDruZnWVG3klrfSzbtZfrzYF4x+sY7fVLsfUY3RSRF84m13hIJPBxhiO3pFPAs6e6zm5GH7B+8Iem1ijIXWNVW5oc7h/Kas77k1h+TcJTVyZ4gL52oqzZM3cwAX0UdE/enNrvYWoeTsJ0UMbNw3bKZ9Ll0BPfdirdHT8Ve7jMzaDF+d11difPOhyZ7wgK3ykzOGu9G8LbzJ8IwUYYFK/1DETYU76XC/d79tUOwSYxGwf88/r2zjn9ZFA7rnzzEnV7ECR33fSoRJALZMyHMUOp8cxa1rYGPrBRyHhivdhhUnyRgXqAq/oymQo4+cwBHZFSpmtEqafQ8RpuOr2ymRgrxBGfe4n4eLprzY5EUZpFRhgxonb10YL16vg/oAlWObdYkS17ZayQtsfbHBD2udjljQXrjWNIWlT6fXG8JeJth+kFewr9+2c0Rfh9sQJ+F2otBk5x+dbt5xTKppAsAEHIy9lN8/Gbh+U+woCxgP11x+w/HYX9KXDkGHcOiAteYEI7Cf2Eo1TKD7ICVTVfReETWxAzSpKMabltNuM8fuLj6dHakvkQ6PgS537ShhyGofbLQaWTB8AMpwRCIUZme6EkfZuoO2CBt8gCnL3U6geDhHUB4ZGU4g9wPL/FlIqSPaWhafwbjc+PCyXqpOMNHdXtNc7D7bAsWN1Nri3Gk1D4ae0BDTunG/SgX4rlx6zc8kGgmFtJ/cnX//RO40Om2Yf36bdeb3KgDo4Ia49EZDaH7FlRn1cwUax0Gr3Jz4=\n    ATUIN_DB_USERNAME: AgAt8yDkKRjmvJtB4ecxOOcuEm1Zcoa8pX1UvtvwAAT4M18PN3JK+6yOyhHuuTwWtWphlQnAjSWx6Bu8usgIxrw9dhBCRxf4pJIaW2VmszUnn1HOtdEFcU6+40PEZ8vJEqCQz/sQoilhZyH06VYecNZFtUHleFAaEFfSGPtxd73lqpjY62fOI8yoGfd/lmXays5vjSx9kUtUVd71FYEOf7P6x+OWlFWsbQ6FepiHygoCXTiCi9umbherpIHWCMZxELja/mNdVZp2wIO+NytedM47LIy2U0FP3b6quPc1H52OK/9AK9TJf/Ke8vUaRDE6TAqv1K0fT5diD4zwERzpNoHKHhnejKj1FOCm6WVcnPHk17zy9Et+kdB+feKpgbeZlolCSJ+JgNWnM2Y3WaovQI4i4yq3ipqQDI1AgY6hHMj1HGNH8gpFjHRy/+UfPd1f4aDO6hGAbL86O2y18VcqD7gESRJ7XVWikJWpU2hIp2FAEpopoqU1QPWyTGvvC46g+gfTARIphn1EzjKymdc4ICb8Viuy/B1oVuwFaD7y9FnNx3tPP4cSuODiG2u6q0j/UTMkAftGqPZUNu3yfkrJHziKUnGc9kuasgAFJKXL2qJuG4VBxNPwTmp2VnJiBysvUb1JTTYd+2uEu4woGmzVfm/9kjkP1rbRk+hAUj5fyW2Nebds9dgD2gXZ2yGOK/S1G0TXnriSQA==\ntemplate:\n    metadata:\n    creationTimestamp: null\n    name: atuin-secrets\n    namespace: atuin\n    type: Opaque\n\n\n---\n# services.yaml\n\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: atuin\nnamespace: atuin\nspec:\ntype: ClusterIP\nports:\n    - port: 8888\n    targetPort: 8888\nselector:\n    app: atuin\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: postgres\nnamespace: atuin\nspec:\ntype: ClusterIP\nports:\n    - port: 5432\n    targetPort: 5432\nselector:\n    app: postgres\n\n---\n# storage.yaml\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: database\nnamespace: atuin\nspec:\naccessModes:\n    - ReadWriteOnce\nresources:\n    requests:\n    storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: atuin-config\nnamespace: atuin\nspec:\naccessModes:\n    - ReadWriteOnce\nresources:\n    requests:\n    storage: 100Mi\n</code></pre>","tags":["homelab"]},{"location":"2025-02-01-uv-resolution/","title":"Test dependency bounds with <code>uv run --resolution</code>","text":"<p>I'm distributing a small Python package at work. A small library with some utilities for doing Machine Learning work. I'm using uv to manage the dependencies and the build process.</p> <p>Part of my pyproject.toml file looks like this:</p> <pre><code>[project]\n...\nrequires-python = \"&gt;=3.10,&gt;3.14\"\ndependencies = [\n    \"pydantic&gt;=2.0,&lt;3\",\n]\n</code></pre> <p>How do I know that my library will work with both <code>pydantic==2.0</code> and <code>pydantic==2.10</code> (The current version at time of writing)? I could just require a much smaller band of possible versions, but I want my library to be useful for as many users as possible. And they might need to use a different version of <code>pydantic</code> for their own projects.</p> <p>Similarly, I want to make sure my library actually works with the range of allowed Python versions.</p> <p>I run my tests with <code>uv run pytest</code>. This will use the locked dependencies in the <code>uv.lock</code> file to create a virtual environment and run the tests in that environment.</p> <p>But, I can use the <code>--resolution</code> flag to test my library with different versions of the dependencies. According to the uv documentation, there are three possible values for the <code>--resolution</code> flag:</p> <ul> <li>highest: Resolve the highest compatible version of each package</li> <li>lowest: Resolve the lowest compatible version of each package</li> <li>lowest-direct: Resolve the lowest compatible version of any direct dependencies, and the highest compatible version of any transitive dependencies</li> </ul> <p>I have found that using <code>--resolution lowest</code> is not really that useful, because some transitive dependencies might not specify a version range. Maybe they just require \"numpy\" without specifying a version. In that case, I will be testing my library against <code>numpy==0.0.1</code> or whatever the lowest version is. That is not really useful. Instead, I use <code>--resolution lowest-direct</code> to test against the lowest version of the direct dependencies and then just select the highest version of the transitive dependencies.</p> <p>I can also specify the python version to use with the <code>--python</code> flag.</p> <p>Finally, I can use the <code>--isolated</code> flag to make sure that the tests are run in an isolated virtual environment, not affecting the active venv of my workspace.</p> <p>Here is the entry in my justfile that runs the tests with different dependency resolutions:</p> justfile<pre><code>test_dependency_bounds:\n    uv run --python 3.10 --resolution lowest-direct --isolated pytest\n    uv run --python 3.13 --resolution highest --isolated pytest\n</code></pre>"},{"location":"2025-02-04-terraform-destructive-pr/","title":"Warn about destructive Terraform changes in pull requests","text":"","tags":["blog","terraform","azure-devops","date/20205/02/04"]},{"location":"2025-02-04-terraform-destructive-pr/#the-problem","title":"The problem","text":"<p>When automating infrastructure changes through CI/CD pipelines, it can be VERY scary to merge a pull request that changes something in your infrastructure that you are not very familiar with.</p> <p>Sure, you have tested the change in a test environment. Did you try to make a plan against a dev environment too? Have you tested against all the relevant targets of this change?</p> <p>Maybe you are reviewing someone else's infrastructure changes. How can you be sure you have caught if this is actually destroying and recreating all the databases?</p> <p>I've dealt with too much of this anxiety! AUTOMATE IT AWAY!</p> <p>With some inspiration from my friend, Lasse Hels, I created this bash script for Azure DevOps Pipelines.</p>","tags":["blog","terraform","azure-devops","date/20205/02/04"]},{"location":"2025-02-04-terraform-destructive-pr/#what-it-does","title":"What it does","text":"<ul> <li>It is assumed to be running as part of a pull request validation pipeline</li> <li>Assuming there is a terraform plan created in the file <code>tfplan</code>, it parses the plan as plaintext and json</li> <li>No matter what, the plaintext plan is posted as a comment in the pull request. The comment will be collapsed by default.</li> <li>If it finds any destructive changes in the plan, the comment will have a big scary warning and be marked as \"Active\". This means someone will have to look at it and resolve it before the pull request can be merged.</li> </ul>","tags":["blog","terraform","azure-devops","date/20205/02/04"]},{"location":"2025-02-04-terraform-destructive-pr/#the-script","title":"The script","text":"<pre><code>#!/bin/bash\nset -euo pipefail\n\n# Somehow create your plan as tfplan\nPLAN_TEXT=$(terraform show tfplan)\nPLAN_JSON=$(terraform show -json tfplan)\nHAS_DESTRUCTIVE_CHANGES=$(echo \"$PLAN_JSON\" | jq -r '.resource_changes[] | select(.change.actions[] | contains(\"delete\"))')\n\n# Conditional alert\nDANGER_MESSAGE=\"\"\nif [ ! -z \"$HAS_DESTRUCTIVE_CHANGES\" ]; then\n DANGER_MESSAGE=\"**DANGER! YOU ARE ABOUT TO DESTROY RESOURCES**\"\nfi\n\n# Actual comment to be posted\nCODE_BLOCK_FENCE='```'\nCOMMENT=$(cat &lt;&lt; EOF\n${DANGER_MESSAGE}\n&lt;details&gt;&lt;summary&gt;Click to expand&lt;/summary&gt;\n\n${CODE_BLOCK_FENCE}\n${PLAN_TEXT}\n${CODE_BLOCK_FENCE}\n&lt;/details&gt;\nEOF\n)\n\n# Set comment status to Active for destructive changes, Resolved otherwise\nCOMMENT_STATUS=2 # Resolved\nif [ ! -z \"$HAS_DESTRUCTIVE_CHANGES\" ]; then\n COMMENT_STATUS=1 # Active\nfi\n\n# Build payload for ADO API\nJSON_PAYLOAD=$(jq -n \\\n --arg content \"$COMMENT\" \\\n --arg status \"$COMMENT_STATUS\" \\\n '{comments: [{content: $content}], status: ($status|tonumber)}'\n)\n\n# Call ADO API to make the comment\ncurl -X POST \\\n \"$(SYSTEM_COLLECTIONURI)/$(SYSTEM_TEAMPROJECT)/_apis/git/repositories/$(BUILD_REPOSITORY_NAME)/pullrequests/$(SYSTEM_PULLREQUEST_PULLREQUESTID)/threads?api-version=6.0\" \\\n -H \"Authorization: Bearer $(SYSTEM_ACCESSTOKEN)\" \\\n -H \"Content-Type: application/json\" \\\n -d \"$JSON_PAYLOAD\"\n</code></pre>","tags":["blog","terraform","azure-devops","date/20205/02/04"]},{"location":"2025-02-04-terraform-destructive-pr/#references","title":"References","text":"<ul> <li>Azure DevOps REST API</li> <li>Azure pipelines variables</li> <li>jq</li> </ul>","tags":["blog","terraform","azure-devops","date/20205/02/04"]},{"location":"2025-02-05-mastodon-rss/","title":"How to follow Mastodon accounts using an RSS feed","text":"<p>I've been really happy with my self-hosted RSS reader FreshRSS . I've consolidated a lot of the newsletter spam and occasional checking of blogs with a more considered decision to go to the RSS reader.</p> <p>This means that I have had less desire to check out whats going on on Mastodon.</p> <p>I just found out that I can get an RSS feed for Mastodon accounts. So, if I wanted to follow @charliemarsh I can just append <code>.rss</code> to the url of his profile webpage. (Note, this has to be his original page). So, Charlies original page is <code>https://hachyderm.io/@charliermarsh</code> and the RSS feed with all his public posts are available at <code>https://hachyderm.io/@charliermarsh.rss</code>.</p> <p>I have started to follow a few Python people where I'm not aware of any blog to follow instead. Aside from Charlie Marsh, I also now follow:</p> <ul> <li>Samuel Colvin</li> <li>Sebasti\u00e1n Ram\u00edrez</li> </ul> <p>I also have an account that I barely use at @KPLauritzen that you can follow.</p> <p>Via Feditips</p>","tags":["blog","mastodon","rss","feed"]},{"location":"2025-02-26-claude-37-strawberry/","title":"Antropics Claude 3.7 Sonnet is released","text":"<p>Yesterday, Anthropic released their new LLM, Claude 3.7 Sonnet.</p> <p>For a run-through of the vibes, I always wait for Zvi Mowshowitz's review.</p> <p>However, there are a few fun highlights here. </p>","tags":["blog","llm","claude","date/2025/02/26"]},{"location":"2025-02-26-claude-37-strawberry/#claudeplayspokemon","title":"ClaudePlaysPokemon","text":"<p>Back in the pre-historic era when I was at Uni, I lost a lot of time to Twitch Plays Pokemon. I was facinated by the idea of a collective playing a game, and the chaos that ensued. A whole mythos got built up, and it was a lot of fun to follow and occasionally spam in the chat. </p> <p>Anthropic has started using \"How far can Claude get in Pokemon\" as one of their benchmarks, but they have also set up an emulator streaming on Twitch.</p> <p></p> <p>You can watch ClaudePlaysPokemon and see how far the model can get.</p>","tags":["blog","llm","claude","date/2025/02/26"]},{"location":"2025-02-26-claude-37-strawberry/#how-many-rs-in-strawberry","title":"How many \"R\"s in Strawberry?","text":"<p>There is an easter egg in the system prompt, where if you ask \"How many R's in Strawberry?\" it will generate a little website that counts the number of R's in the word strawberry.</p> <p>I have pasted the HTML directly below (simultaneously testing how markdown with HTML gets handled by MkDocs)</p> \ud83c\udf53 Click the strawberry to count!","tags":["blog","llm","claude","date/2025/02/26"]},{"location":"2025-11-22-homelab-metrics/","title":"Monitoring usage metrics on the homelab","text":"<p>I have a very small and underpowered kubernetes cluster at home (see Homelab #1 and Homelab #2), and I'm always concerned that I'm overutilizing its resources when I want to add another service to it. </p> <p>For example, sometimes my FreshRSS site is not responding. Is that because there is a network problem, or am I running out of memory, or something completely different. </p> <p>One option could be to run Prometheus and Grafana, scraping metrics from each service at regular intervals. The problem with that is that I'm already running on a low-power system, so spending a significant chunk of the total resources only on MONITORING how many resources I'm using seems like a bad idea. </p> <p>Instead, partly inspired by Markus' love for SQLite, I'm doing a much simpler thing: </p> <ol> <li>Run <code>kubectl top nodes</code> and <code>kubectl top pods</code> every 5 minutes with an entry in <code>crontab</code>. </li> <li>Do a very simple parsing of the output</li> <li>Insert it into a SQLite database with a timestamp. </li> </ol> <p>The database is a local file, stored on the same computer running <code>k3s</code>. When I want to look at how my system has been behaving recently, I can copy the whole database to my desktop and start digging into it. </p> <p>No fuss, minimal overhead, no fancy dashboard. It's weird to do YAGNI on my homelab, because I don't NEED any of it. </p>","tags":["homelab","metrics","grafana"]},{"location":"2025-11-22-homelab-metrics/#appendix","title":"Appendix:","text":"<p>The script I use to collect metrics: <pre><code>#!/bin/bash\nset -euo pipefail\n\n# Database location\nDB_PATH=\"${DB_PATH:-/var/lib/k8s-metrics/metrics.db}\"\nTIMESTAMP=$(date -u +\"%Y-%m-%d %H:%M:%S\")\n\n# Full path to kubectl (k3s)\nKUBECTL=\"${KUBECTL:-/usr/local/bin/kubectl}\"\n\n# Ensure database directory exists\nmkdir -p \"$(dirname \"$DB_PATH\")\"\n\n# Initialize database if it doesn't exist\nsqlite3 \"$DB_PATH\" &lt;&lt;EOF\nCREATE TABLE IF NOT EXISTS node_metrics (\n    timestamp TEXT NOT NULL,\n    node_name TEXT NOT NULL,\n    cpu_cores TEXT NOT NULL,\n    cpu_percent TEXT NOT NULL,\n    memory_bytes TEXT NOT NULL,\n    memory_percent TEXT NOT NULL,\n    PRIMARY KEY (timestamp, node_name)\n);\n\nCREATE TABLE IF NOT EXISTS pod_metrics (\n    timestamp TEXT NOT NULL,\n    namespace TEXT NOT NULL,\n    pod_name TEXT NOT NULL,\n    cpu_cores TEXT NOT NULL,\n    memory_bytes TEXT NOT NULL,\n    PRIMARY KEY (timestamp, namespace, pod_name)\n);\n\nCREATE INDEX IF NOT EXISTS idx_node_timestamp ON node_metrics(timestamp);\nCREATE INDEX IF NOT EXISTS idx_pod_timestamp ON pod_metrics(timestamp);\nEOF\n\n# Collect node metrics\n$KUBECTL top nodes --no-headers | while read -r name cpu cpu_pct memory memory_pct; do\n    sqlite3 \"$DB_PATH\" &lt;&lt;EOF\nINSERT INTO node_metrics (timestamp, node_name, cpu_cores, cpu_percent, memory_bytes, memory_percent)\nVALUES ('$TIMESTAMP', '$name', '$cpu', '$cpu_pct', '$memory', '$memory_pct');\nEOF\ndone\n\n# Collect pod metrics\n$KUBECTL top pods -A --no-headers | while read -r namespace name cpu memory; do\n    sqlite3 \"$DB_PATH\" &lt;&lt;EOF\nINSERT INTO pod_metrics (timestamp, namespace, pod_name, cpu_cores, memory_bytes)\nVALUES ('$TIMESTAMP', '$namespace', '$name', '$cpu', '$memory');\nEOF\ndone\n\necho \"Metrics collected at $TIMESTAMP\"\n</code></pre></p> <p>The crontab entry <pre><code>*/5 * * * * KUBECONFIG=/home/kasper/.kube/config DB_PATH=/home/kasper/k8s-metrics/metrics.db /home/kasper/k8s-metrics/collect-metrics.sh &gt;&gt; /home/kasper/k8s-metrics/collect.log 2&gt;&amp;1\n</code></pre></p>","tags":["homelab","metrics","grafana"]},{"location":"2025-11-25-reading-rss-feeds/","title":"Reading RSS feeds","text":"<p>My good friend Zeth just posted on using RSS feeds to reduce distractions and I think that is super interesting! He is always running small experiments on himself to optimize or nudge his behaviour in some way. Here, here is trying out RSS feeds. Go read his post now.</p>","tags":["rss"]},{"location":"2025-11-25-reading-rss-feeds/#why-rss","title":"Why RSS","text":"<p>Not being easily distracted seems like a core skill today. I'm using RSS readers to reduce the slot machine behaviour from clicking on a bookmark hoping for a new exciting post. Now there is just one place to go, and I can more easily control my cravings.</p> <p>I'm also using RSS readers as a way of being less reliant on someone else's algorithm deciding what I get to see. Some RSS readers push AI filtering or \"trending topics\", as Zeth mentions. I don't like that! Avoiding the algorithm means that curation is only up to you.</p> <p>I'm using FreshRSS self-hosted in a one-node Kubernetes cluster in my home lab. I like that it \"gets out of the way\" quickly. It is just a chronological list of posts, with some simple options to see e.g. all posts from a certain feed or category.</p> <p>Full disclosure: I also still have a very bad habit of opening Hacker News whenever I'm stuck on something at work. Do as I say, not as I do-</p>","tags":["rss"]},{"location":"2025-11-25-reading-rss-feeds/#good-rss-reading-habits","title":"Good RSS reading habits","text":"<p>Find good feeds to subscribe to! By far the most important thing, having high-quality stuff to read. I'm collecting my recommendations. Mostly people I actually know and then a few other sources I really like. It's easy to add a news feed that pushes 20 news stories every day. That defeats the whole purpose. You will be drowning in noise.</p> <p>Prune your feed I have some sources that I have added due to a particularly high-quality post, and if I haven't been impressed with their writing over the next weeks or months, I will remove that feed again.</p> <p>Don't try to read everything I think this one is important as well. You don't owe any of these writers your attention. When I ignore or forget my RSS reader for a week (or a month), and then get back to dozens or hundreds of unread stories, I will usually just mark everything older than some date \"read\". Maybe there was some gold hiding there, but there is no way I'm going to find it in the pile of random stuff. The internet is not lacking in content. Trying to read everything is a permanent backlog out of your control. Avoid that!</p> <p>Don't feel obligated to read the post completely I practice this mostly for Zvi's notoriously long AI newsletters, but it is also true for anything else. They are very interesting, and I enjoy them a lot, but not enough to read a short novel several times a week. Skim, skip, consider asking your AI if there are sections you should focus on given your interests. Zvi also marks the most interesting sections with a bold heading in the Table of Contents. Thanks!</p>","tags":["rss"]},{"location":"2025-12-15-fred-stats-rant/","title":"Fred","text":"<p>My good friend, Fred, has started blogging on substack recently. </p> <p>Fred has MANY skills. He has been DOING things, been poking at the world with every stick he has ever found, changed careers to something more exciting more times than I can easily express.</p> <p>He could probably express it. One skill he has, maybe the thing he is the best at, is writing. </p> <p>Actually, there is at least one thing I'll rank higher than Fred's writing. That is a good Fred rant. And his blog feels like a series of particularly high-quality rants, interspersed with actual, honest-to-god, advice on being a good human. </p> <p>So I'll drop whatever else I'm doing whenever I see a [FRED] post in my RSS feed, get comfy in my chair, and enjoy the fireworks. </p> <p>You might need an example: How about this one, opening on the toilet at the therapists office, setting the scene like this</p> <p>I\u2019ve just suffered through part bowel movement, part catholic exorcism\u2026</p> <p>Resulting in one of those near-mythological Fantasy Epic wipes that need more paper than all of Robert Jordan\u2019s Wheel Of Time series.</p> <p>This is a Sisyphean task straight out of Greek mythology - pushing a brown tar-like with inexplicable-grains-in-it boulder uphill every day until the world runs out of patience and paper.</p> <p>IS THAT A HOOK OR WHAT?</p> <p>Ok, so he is also incredibly crass. That just makes it better. </p>","tags":["fred"]},{"location":"2025-12-15-fred-stats-rant/#on-having-a-gooch-day","title":"On having a Gooch Day","text":"<p>His most recent post is about productivity. </p> <p>I think you should read the post, but the shortest possible summary is: - Suppose your daily produtivity is following a truncated normal distribution, from 0% to 100% productive days.  - Then you will have a few days in a month that feel very productive, a few that feel not-at-all productive, and most days will be in-between. Those are the Gooch Days.</p> <p>Fred points out that you can't rely on the super productive days, they are too rare. Instead, the real hero is the Gooch. This is where progress is made, because this is where the volume is at. </p> <p>I wanted to play around with this intuition a little bit, and since Python is my hammer and everything is a nail, I fired up some matplotlib. </p> <p>Given a normal distribution with a mean of 50% and a standard deviation of 20%, we can approximate the histogram Fred showed. And if we want to know how much each day contributes to the total productivity, we can plot $x \\times p(x)$, the expected contribution. For me, it helped to think of Expected Value calculations: Imagine you make a bet that will give 100 EUR if you win, and you have a 20% chance to win. The expected value is $100 \\times 0.2 = 20$ EUR.</p> <p>So now the height on the histogram is not representing the frequency of days with that amount of productivity. Instead it represents how much those days contribute to the overall productivity (of the month/year/whatever).</p> <p>I've plotted both the assumed productivy distribution and the contribution below. </p> <p></p> <p>Note the area with a slightly off-putting color, between 20% and 80% productivity, the Gooch Days. All together, they represent 87% of your total productivity! They are basically all that matter. </p>","tags":["fred"]},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"archive/2023/","title":"2023","text":""},{"location":"archive/2021/","title":"2021","text":""},{"location":"archive/2019/","title":"2019","text":""},{"location":"archive/2018/","title":"2018","text":""},{"location":"page/2/","title":"What I've been up to","text":""}]}